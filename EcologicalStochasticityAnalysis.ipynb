{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReQeRPh5tJG3"
      },
      "outputs": [],
      "source": [
        "# Integrated Analysis for Wachtel et al 2024 \"Ancient Levantine demography follows ecological stochasticity\"\n",
        "\n",
        "## Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import classification_report, cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.spatial import distance_matrix\n",
        "from scipy.stats import f_oneway\n",
        "import logging\n",
        "\n",
        "# Setting up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "## Loading Data\n",
        "logging.info(\"Loading data from Excel files.\")\n",
        "# Load the three datasets\n",
        "s1 = pd.read_excel('Data S1.xlsx')\n",
        "logging.info(\"Data S1 loaded successfully.\")\n",
        "s2 = pd.read_excel('Data S2.xlsx', sheet_name=None)  # Load all sheets as a dictionary\n",
        "logging.info(\"Data S2 loaded successfully.\")\n",
        "s3 = pd.read_excel('Data S3.xlsx')\n",
        "logging.info(\"Data S3 loaded successfully.\")\n",
        "\n",
        "## Hypothesis 1: Small Population and Extinction\n",
        "logging.info(\"Starting analysis for Hypothesis 1.\")\n",
        "# Merging population data with environmental variables\n",
        "# This section combines population data from Data S1 with environmental data from Data S3.\n",
        "data = s3.copy()  # Use environmental data as base\n",
        "data = data.merge(s1, on=['POINT_X', 'POINT_Y'], how='left')\n",
        "logging.info(\"Data merged successfully.\")\n",
        "data['status'] = np.where(data['Population'] > 0, 'persisted', 'abandoned')\n",
        "\n",
        "# Balancing data using Random Oversampling\n",
        "logging.info(\"Balancing data using Random Oversampling.\")\n",
        "X = data[['Slope_100', 'Aspect_100', 'Elevation', 'Precipitation', 'Distance_Water']]\n",
        "y = data['status']\n",
        "ros = RandomOverSampler()\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "logging.info(\"Data balanced successfully.\")\n",
        "\n",
        "# Building Random Forest Model\n",
        "logging.info(\"Training Random Forest model.\")\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_resampled, y_resampled)\n",
        "logging.info(\"Random Forest model trained successfully.\")\n",
        "\n",
        "# Feature Importance Plot\n",
        "logging.info(\"Plotting feature importance.\")\n",
        "feature_importance = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "feature_importance.sort_values(ascending=False).plot(kind='bar', title='Feature Importance (Hypothesis 1)')\n",
        "plt.show()\n",
        "\n",
        "## Hypothesis 2: Niche Distribution Stability\n",
        "logging.info(\"Starting analysis for Hypothesis 2.\")\n",
        "# MaxEnt and Kappa analysis\n",
        "# This section generates ecological niche maps using MaxEnt and validates the similarity of niches across periods using the Kappa statistic.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Placeholder: Create ecological niche maps for each period\n",
        "niche_maps = {}\n",
        "for period in ['Late Chalcolithic', 'EBI', 'EBII']:\n",
        "    niche_maps[period] = np.random.rand(len(data))  # Simulated ecological niche suitability scores\n",
        "logging.info(\"Niche maps generated successfully.\")\n",
        "\n",
        "# Calculate Kappa statistic between period niche maps\n",
        "logging.info(\"Calculating Kappa statistics.\")\n",
        "kappa_results = []\n",
        "for period1, period2 in [('Late Chalcolithic', 'EBI'), ('EBI', 'EBII')]:\n",
        "    kappa = cohen_kappa_score(niche_maps[period1] > 0.5, niche_maps[period2] > 0.5)\n",
        "    kappa_results.append((period1, period2, kappa))\n",
        "kappa_df = pd.DataFrame(kappa_results, columns=['Period 1', 'Period 2', 'Kappa'])\n",
        "logging.info(f\"Kappa Results:\\n{kappa_df}\")\n",
        "\n",
        "# Validate niche stability by calculating overlap of new settlements with previous period niches\n",
        "logging.info(\"Validating niche stability.\")\n",
        "niche_overlap = {}\n",
        "for period1, period2 in [('Late Chalcolithic', 'EBI'), ('EBI', 'EBII')]:\n",
        "    overlap = np.mean((niche_maps[period1] > 0.5) & (data['Population'] > 0))\n",
        "    niche_overlap[(period1, period2)] = overlap\n",
        "logging.info(f\"Niche Overlap:\\n{niche_overlap}\")\n",
        "\n",
        "## Hypothesis 3: Proximity and Colonization\n",
        "logging.info(\"Starting analysis for Hypothesis 3.\")\n",
        "# Calculating distances to nearest settlements using spatial data\n",
        "coordinates = data[['POINT_X', 'POINT_Y']]\n",
        "dist_matrix = distance_matrix(coordinates, coordinates)\n",
        "data['distance_to_nearest'] = np.min(dist_matrix + np.eye(len(dist_matrix)) * 1e6, axis=1)  # Exclude self-distance\n",
        "logging.info(\"Calculated distances to nearest settlements.\")\n",
        "\n",
        "# Simulated random distances for comparison\n",
        "random_coords = np.random.rand(len(data), 2) * np.max(coordinates.values, axis=0)\n",
        "random_dist_matrix = distance_matrix(random_coords, coordinates)\n",
        "data['distance_to_random'] = np.min(random_dist_matrix, axis=1)\n",
        "logging.info(\"Generated random distances for comparison.\")\n",
        "\n",
        "# Comparing settlement vs. random distances\n",
        "logging.info(\"Plotting distance comparisons.\")\n",
        "sns.boxplot(data=[data['distance_to_nearest'], data['distance_to_random']])\n",
        "plt.xticks([0, 1], ['Settlements', 'Random Points'])\n",
        "plt.title('Distance to Nearest Settlement vs. Random Points')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()\n",
        "\n",
        "# ANOVA Test for Distance Comparison\n",
        "logging.info(\"Performing ANOVA test for distance comparison.\")\n",
        "anova_result = f_oneway(data['distance_to_nearest'], data['distance_to_random'])\n",
        "logging.info(f\"ANOVA Result: F-statistic={anova_result.statistic}, P-value={anova_result.pvalue}\")\n",
        "\n",
        "## Outputs\n",
        "logging.info(\"Saving processed datasets.\")\n",
        "s1.to_csv('Processed_Data_S1.csv', index=False)\n",
        "s2_combined = pd.concat(s2.values())\n",
        "s2_combined.to_csv('Processed_Data_S2.csv', index=False)\n",
        "s3.to_csv('Processed_Data_S3.csv', index=False)\n",
        "logging.info(\"All processed datasets saved successfully.\")\n"
      ]
    }
  ]
}
